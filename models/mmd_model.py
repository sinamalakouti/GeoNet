from torchvision import models
import torch.nn as nn
import torch

class BottleNeckFeatureExtractor(nn.Module):
    """
    On the original paper they have used a CNN + three fully connected layers of 4096,
    4096, n_classes ad added a lower dimensional 'bottleneck', adaptation layer.
    Here we used a ResNet + FFL {2056, 2056, 512, n_classes} and we will compute
    the MMD on the output of the 512 layer.

    Input arguments:
      dropout: float (.15) = probability of dropout.
      out_dim: int (512) = feature dimension out.
    """

    def __init__(
            self,
            dropout: float = .15,
            out_dim: int = 512
    ) -> None:
        super().__init__()
        self.feature_extractor = models.resnet50(pretrained=True)
        in_features_head = self.feature_extractor.fc.in_features
        self.feature_extractor.fc = nn.Identity()

        # heavy linear layers
        self.dense_layer = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(in_features_head, in_features_head),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(in_features_head, in_features_head),
            nn.Dropout(dropout),
            nn.ReLU(),
        )

        # bottle neck linear layers
        self.bottle_neck = nn.Sequential(
            nn.Linear(in_features_head, out_dim),
            nn.Dropout(dropout),
            # nn.Tanh(), trial
            nn.ReLU(),
        )
        self.name = "BottleNeckFeatureExtractor"
        self.to(
            torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        )

    def forward(self, x):
        feature_map = self.feature_extractor(x)
        dense_representation = self.dense_layer(feature_map)
        shrinked_feature_map = self.bottle_neck(dense_representation)
        return shrinked_feature_map


class MMDClassifer(nn.Module):
    """
    MMD classification headclassifier composed by a simple Linear Layer.
    the features that comes in are already passed throughÂ an activation function.

    Input arguments:
        dropout: float (.15) = probability of dropout.
        out_dim: int (512) = feature in from the feature_extractor.
        num_classes: int (20) = dimension of final output.
    """

    def __init__(
            self,
            dropout: float = .15,
            out_dim: int = 512,
            num_classes: int = 600
    ) -> None:
        super().__init__()

        self.classifier_head = nn.Linear(out_dim, num_classes)
        self.name = "MMDClassifer"
        self.to(
            torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        )

    def forward(self, shrinked_feature_map):
        output = self.classifier_head(shrinked_feature_map)
        return output